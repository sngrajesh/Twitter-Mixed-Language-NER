{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb45b3de-9a94-4f97-b239-26a218ad58e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mirascope\n",
      "  Downloading mirascope-1.16.7-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: pydantic in /home/dai/anaconda3/lib/python3.12/site-packages (2.8.2)\n",
      "Requirement already satisfied: groq in /home/dai/anaconda3/lib/python3.12/site-packages (0.18.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from mirascope)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting jiter>=0.5.0 (from mirascope)\n",
      "  Downloading jiter-0.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/dai/anaconda3/lib/python3.12/site-packages (from mirascope) (4.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/dai/anaconda3/lib/python3.12/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/dai/anaconda3/lib/python3.12/site-packages (from pydantic) (2.20.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/dai/anaconda3/lib/python3.12/site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/dai/anaconda3/lib/python3.12/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/dai/anaconda3/lib/python3.12/site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /home/dai/anaconda3/lib/python3.12/site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/dai/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in /home/dai/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/dai/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/dai/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Downloading mirascope-1.16.7-py3-none-any.whl (309 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading jiter-0.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "Installing collected packages: jiter, docstring-parser, mirascope\n",
      "Successfully installed docstring-parser-0.16 jiter-0.8.2 mirascope-1.16.7\n"
     ]
    }
   ],
   "source": [
    "!pip install mirascope pydantic groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1590972-4b2d-410b-a6c0-8ef1c3a3bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "gu_input = \"./input/train.gu\"\n",
    "mr_input = \"./input/train.mr\"\n",
    "te_input = \"./input/train.te\"\n",
    "en_input = \"./input/train.en\"\n",
    "hi_input = \"./input/train.hi\"\n",
    "\n",
    "gu_output = \"./output_sentence_ner/out_gu.json\"\n",
    "mr_output = \"./output_sentence_ner/out_mr.json\"\n",
    "te_output = \"./output_sentence_ner/out_te.josn\"\n",
    "en_output = \"./output_sentence_ner/out_en.json\"\n",
    "hi_output = \"./output_sentence_ner/out_hi.josn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e275624-e1cf-4a83-a02e-b57be5e038b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from groq import Groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbab86-2515-4057-9a00-44a77e04e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "api_key = os.getenv('API_KEY')\n",
    "groq_client = Groq(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37696dae-2fba-4574-a2d2-10a2aac6ad50",
   "metadata": {},
   "source": [
    "---\n",
    "$$\n",
    "\\Large\\text{Word level NER}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60dd1af4-45b3-410c-a1fc-6694f128e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ner_ollama_local(sentence):\n",
    "    url = \"http://localhost:11434/v1/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": \"llama3.1:latest\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "                Extract named entities (Person, Organization, Location, Event, Product) from the given sentence.\n",
    "                and give the output in following format\n",
    "                {\n",
    "                    \"B-PER\": [],\n",
    "                    \"I-PER\": [],\n",
    "                    \"B-ORG\": [],\n",
    "                    \"I-ORG\": [],\n",
    "                    \"B-LOC\": [],\n",
    "                    \"I-LOC\": [],\n",
    "                    \"B-EVT\": [],\n",
    "                    \"I-EVT\": [],\n",
    "                    \"B-PROD\": [],\n",
    "                    \"I-PROD\": [],\n",
    "                    \"Others\"[]\n",
    "                }\n",
    "                all the words which are not named entities should be in Others\n",
    "                Dont write anything besides response\n",
    "                \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": sentence}\n",
    "        ]\n",
    "    } \n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Request failed:\", e)\n",
    "        return {}\n",
    "\n",
    "    response_json = response.json()\n",
    "    try:\n",
    "        response_content = response_json[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except (KeyError, IndexError) as e:\n",
    "        print(\"Unexpected response structure:\", response_json)\n",
    "        return {}\n",
    "\n",
    "    if not response_content:\n",
    "        print(\"Empty response\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        ner_data = json.loads(response_content)\n",
    "        return ner_data\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error parsing JSON response:\")\n",
    "        print(response_content)\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "138f3046-3776-4c31-9d4b-af60aab570e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ner(sentence):\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "                Extract named entities (Person, Organization, Location, Event, Product) into two categories (B and  I) from the given sentence.\n",
    "                and give the output in following format\n",
    "                {\n",
    "                    \"B-PER\": [],\n",
    "                    \"I-PER\": [],\n",
    "                    \"B-ORG\": [],\n",
    "                    \"I-ORG\": [],\n",
    "                    \"B-LOC\": [],\n",
    "                    \"I-LOC\": [],\n",
    "                    \"B-EVT\": [],\n",
    "                    \"I-EVT\": [],\n",
    "                    \"B-PROD\": [],\n",
    "                    \"I-PROD\": [],\n",
    "                    \"Others\"[]\n",
    "                }\n",
    "                all the words which are not named entities should be in Others\n",
    "                Dont write anything besides response\n",
    "                \"\"\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": sentence}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response_content = response.choices[0].message.content.strip()\n",
    "    if not response_content:\n",
    "        print(\"Empty response\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        ner_data = json.loads(response_content)\n",
    "        return ner_data\n",
    "    except json.JSONDecodeError:\n",
    "        print(response_content)\n",
    "        print(\"Error parsing JSON response\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1540c880-6e38-42e6-9dbc-9e9e02afb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, filename=\"ner_output.csv\"):\n",
    "    if not data:\n",
    "        return\n",
    "\n",
    "    with open(filename, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"Entity\", \"Type\"])\n",
    "        for entity_type, entities in data.items():\n",
    "            for entity in entities:\n",
    "                writer.writerow([entity, entity_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13a5dfde-79b0-4be4-ae80-9329268c9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_and_save_to_csv(input_file, output_file, start=0, end=10000, step=1):\n",
    "    with open(input_file, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        sentences = file.readlines()\n",
    "\n",
    "    for i in range(start, end, step):\n",
    "        merged_sentence = \"\".join(sentences[i:i+step])\n",
    "        ner_result = perform_ner(merged_sentence)\n",
    "        save_to_csv(ner_result, output_file)\n",
    "        print(i+step)\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019c125-2e3b-4564-a31f-f68c067c240b",
   "metadata": {},
   "source": [
    "---\n",
    "$$\n",
    "\\Large\\text{Sentences NER}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c078579-b0bb-4653-8cb9-6e33b6470c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ner_sentence(sentence):\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "                Extract named entities (Person, Organization, Location, Event, Product) into two categories (B and I) from the given sentence.\n",
    "                and give the output in the following format.\n",
    "                Example input: \"महेन्द्र साळुंखे यांनी ताज प्रोडक्शन कडून पुण्यातील 'मराठी महोत्सव' मध्ये नवीन स्मार्टवॉच प्रदर्शित केली.\" \n",
    "                Output:\n",
    "                [\n",
    "                    [\"महेन्द्र\", \"साळुंखे\", \"यांनी\", \"ताज\", \"प्रोडक्शन\", \"कडून\", \"पुण्यातील\", \"'मराठी महोत्सव'\", \"मध्ये\", \"नवीन स्मार्टवॉच\", \"प्रदर्शित\", \"केली.\"],\n",
    "                    [\"B-PER\", \"I-PER\", 0, \"B-ORG\", \"I-ORG\", 0, \"B-LOC\", \"B-EVENT\", 0, \"B-PROD\", 0, 0]\n",
    "                ]\n",
    "                All words that are not named entities should be marked as 0.\n",
    "                Strictly output nothing but the response.\n",
    "                \"\"\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": sentence}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response_content = response.choices[0].message.content.strip()\n",
    "    if not response_content:\n",
    "        print(\"Empty response\")\n",
    "        return []\n",
    "\n",
    "    if not response_content.startswith(\"[\"):\n",
    "        # counting 3*[ and 3*] brackes then process\n",
    "         if response_content.count('[') != 3 or response_content.count(']') != 3:\n",
    "            print(\"Invalid response format\")\n",
    "            return []\n",
    "        # string from fisrt occurance of [ to las occurance of ]\n",
    "         start = response_content.index('[')\n",
    "         end = response_content.rindex(']')\n",
    "         stripped = response_content[start:end+1]\n",
    "         response_content = stripped\n",
    " \n",
    "    try:\n",
    "        ner_data = ast.literal_eval(response_content)\n",
    "    except Exception as e:\n",
    "        print(response_content)\n",
    "        print(\"Error parsing response:\", e)\n",
    "        return []\n",
    "\n",
    "    # print(response_content)\n",
    "    \n",
    "    # Validate that ner_data is a list of exactly two lists.\n",
    "    if not (isinstance(ner_data, list) and len(ner_data) == 2):\n",
    "        print(\"Response format error: Expected a list with two elements.\")\n",
    "        return []\n",
    "\n",
    "    tokens, labels = ner_data\n",
    "    if not (isinstance(tokens, list) and isinstance(labels, list)):\n",
    "        print(\"Response format error: Both elements must be lists.\")\n",
    "        return []\n",
    "\n",
    "    if len(tokens) != len(labels):\n",
    "        diff = len(tokens) - len(labels)\n",
    "        if abs(diff) == 1:\n",
    "            if diff == 1:\n",
    "                # tokens list is longer by one; append 0 to labels\n",
    "                labels.append(0)\n",
    "            elif diff == -1:\n",
    "                # labels list is longer by one; insert an empty string at the beginning of tokens\n",
    "                tokens.insert(0, \"\")\n",
    "        else:\n",
    "            print(\"Response format error: Tokens and labels length mismatch.\")\n",
    "            return []\n",
    "\n",
    "    return [tokens, labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0791af0c-9f6b-4270-8a67-c0f759b019a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\"Elon\", \"Musk\", \"is\", \"the\", \"CEO\", \"of\", \"Tesla,\", \"and\", \"he\", \"was\", \"born\", \"in\", \"Pretoria,\", \"South\", \"Africa.\"],\n",
      "    [\"B-PER\", \"I-PER\", 0, 0, 0, 0, \"B-ORG\", 0, 0, 0, 0, \"B-LOC\", \"I-LOC\", \"I-LOC\", 0]\n",
      "]\n",
      "[['Elon', 'Musk', 'is', 'the', 'CEO', 'of', 'Tesla,', 'and', 'he', 'was', 'born', 'in', 'Pretoria,', 'South', 'Africa.'], ['B-PER', 'I-PER', 0, 0, 0, 0, 'B-ORG', 0, 0, 0, 0, 'B-LOC', 'I-LOC', 'I-LOC', 0]]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Elon Musk is the CEO of Tesla, and he was born in Pretoria, South Africa.\"\n",
    "# sentence = \"Barack Obama visited Microsoft headquarters in Seattle during a tech conference.\"\n",
    "# sentence = \"તમારા ઘરના નેર મારે એક બગીચો છે.\"\n",
    "# print(perform_ner(sentence))\n",
    "print(perform_ner_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ece9d6d-6da2-460c-9128-cb8d003b29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(ner_result, output_file):\n",
    "    \"\"\"\n",
    "    Save the ner_result (expected to be a list of two lists) to the output file. Each result is saved on a new line in JSON format.\n",
    "    \"\"\"\n",
    "    with open(output_file, mode=\"a\", encoding=\"utf-8\") as f:\n",
    "        result_str = json.dumps(ner_result, ensure_ascii=False)\n",
    "        f.write(result_str + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccdaa0f-df51-437a-b454-3ab3abca0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_and_save_to_csv(input_file, output_file, start=0, end=10000):\n",
    "    with open(input_file, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        sentences = file.readlines()\n",
    "\n",
    "    for i in range(start, end):\n",
    "        ner_result = perform_ner_sentence(sentences[i])\n",
    "        try:\n",
    "            if len(ner_result) == 2:\n",
    "                save_to_file(ner_result, output_file)\n",
    "                print('S: ',i)\n",
    "            else:\n",
    "                print('E: ',i)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('E: ',i)\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d057a75-6e98-49f7-be60-f692ec160811",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_and_save_to_csv(mr_input, mr_output,0,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
